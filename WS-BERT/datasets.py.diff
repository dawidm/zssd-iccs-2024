101a102,183
> class SemEval(Dataset):
>     def __init__(self, phase, topic, model='bert-base', wiki_model=''):
>         path = 'data/semeval'
>         file_path = f'{path}/{topic}_{phase}.csv'
>         df = pd.read_csv(file_path)
>         print(f'# of {phase} examples: {df.shape[0]}')
> 
>         topic = topic.replace('_', ' ')
> 
>         tweets = df['Tweet'].tolist()
>         targets = df['Target'].tolist()
>         stances = df['Stance'].map({'AGAINST': 0, 'FAVOR': 1, 'NONE': 2}).tolist()
> 
>         os.environ['TRANSFORMERS_OFFLINE'] = '0'
>         from transformers import AutoTokenizer
>         if model == 'bert-base':
>             tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
>         elif model == 'bertweet':
>             tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')
>         else:  # 'covid-twitter-bert':
>             tokenizer = AutoTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert-v2')
> 
>         if wiki_model:
>             wiki_dict = pickle.load(open(f'{path}/wiki_dict.pkl', 'rb'))
>             wiki_summary = wiki_dict[topic]
> 
>             if wiki_model == model or wiki_model == 'merge':
>                 tokenizer_wiki = tokenizer
>             else:
>                 if wiki_model == 'bert-base':
>                     tokenizer_wiki = AutoTokenizer.from_pretrained('bert-base-uncased')
>                 elif wiki_model == 'bertweet':
>                     tokenizer_wiki = AutoTokenizer.from_pretrained('vinai/bertweet-base')
>                 else:  #  'covid-twitter-bert':
>                     tokenizer_wiki = AutoTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert-v2')
> 
>             if wiki_model == model:
>                 tweets_targets = [f'text: {x} target: {y}' for x, y in zip(tweets, targets)]
>                 encodings = tokenizer(tweets_targets, [wiki_summary] * df.shape[0], padding=True, truncation=True)
>                 encodings_wiki = {'input_ids': [[0]] * df.shape[0], 'attention_mask': [[0]] * df.shape[0]}
>             else:
>                 encodings = tokenizer(tweets, targets, padding=True, truncation=True)
>                 encodings_wiki = tokenizer_wiki([wiki_summary] * df.shape[0], padding=True, truncation=True)
> 
>         else:
>             encodings = tokenizer(tweets, targets, padding=True, truncation=True)
>             encodings_wiki = {'input_ids': [[0]] * df.shape[0], 'attention_mask': [[0]] * df.shape[0]}
> 
>         # encodings for the texts and tweets
>         input_ids = torch.tensor(encodings['input_ids'], dtype=torch.long)
>         attention_mask = torch.tensor(encodings['attention_mask'], dtype=torch.long)
>         token_type_ids = torch.tensor(encodings['token_type_ids'], dtype=torch.long) if model != 'roberta' else torch.zeros(df.shape[0])
> 
>         # encodings for wiki summaries
>         input_ids_wiki = torch.tensor(encodings_wiki['input_ids'], dtype=torch.long)
>         attention_mask_wiki = torch.tensor(encodings_wiki['attention_mask'], dtype=torch.long)
> 
>         stances = torch.tensor(stances, dtype=torch.long)
>         print(f'max len: {input_ids.shape[1]}, max len wiki: {input_ids_wiki.shape[1]}')
> 
>         self.input_ids = input_ids
>         self.attention_mask = attention_mask
>         self.token_type_ids = token_type_ids
>         self.stances = stances
>         self.input_ids_wiki = input_ids_wiki
>         self.attention_mask_wiki = attention_mask_wiki
> 
>     def __getitem__(self, index):
>         item = {
>             'input_ids': self.input_ids[index],
>             'attention_mask': self.attention_mask[index],
>             'token_type_ids': self.token_type_ids[index],
>             'stances': self.stances[index],
>             'input_ids_wiki': self.input_ids_wiki[index],
>             'attention_mask_wiki': self.attention_mask_wiki[index]
>         }
>         return item
> 
>     def __len__(self):
>         return self.stances.shape[0]
> 
> 
187c269
<             file_path = f'{path}/vast_{phase}.csv'
---
>             file_path = f'{path}/vast_{phase}_corrected.csv'
189c271
<             file_path = f'{path}/vast_dev.csv'
---
>             file_path = f'{path}/vast_dev_corrected.csv'
293a376,377
>     elif data == 'semeval':
>         dataset = SemEval(phase, topic, model=model, wiki_model=wiki_model)

